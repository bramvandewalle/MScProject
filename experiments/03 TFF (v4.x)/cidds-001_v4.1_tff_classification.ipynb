{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bram_\\home\\msc\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General overview of running federated learning with Tensorflow Federated\n",
    "\n",
    "## Run a single round of Federated Averaging:\n",
    "```python\n",
    "    state, metrics = iterative_process.next(state, federated_training_data)\n",
    "```\n",
    "\n",
    "## Initialize a _state_ before first round of Federated Averaging:\n",
    "```python\n",
    "    state = iterative_process.initialize()\n",
    "```\n",
    "\n",
    "## The _iterative_process_ is an object obtained by the method _tff.learning.build_federated_averaging_process_:\n",
    "```python\n",
    "    iterative_process = tff.learning.build_federated_averaging_process(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "## The _model_fn_ is a function that creates a new keras model and converts it to a _tff.learning.Model object_:\n",
    "```python\n",
    "    def model_fn():\n",
    "        keras_model = create_keras_model()\n",
    "        return tff.learning.from_keras_model(\n",
    "    \t    keras_model,\n",
    "    \t    input_spec=preprocessed_exampe_dataset.element_spec,\n",
    "    \t    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \t    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "```\n",
    "\n",
    "## The keras model is created via an auxiliary method _create_keras_model_ which needs to create a new model:\n",
    "```python\n",
    "    def create_keras_model():\n",
    "        return tf.keras.models.Sequential([\n",
    "            ...\n",
    "        ])\n",
    "```\n",
    "\n",
    "## Create the _federated_train_data_ object\n",
    "The `federated_train_data` object is a list of `tf.data.Dataset` objects. For each participating client, there is one `tf.data.Dataset` object. How to create one should be described [here](https://www.tensorflow.org/guide/data) according to [this](https://www.py4u.net/discuss/166632) blog post. Another interesting post was [here](https://stackoverflow.com/a/55462901/9356123) on StackOverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "\n",
    "# tensorflow_federated_nightly also bring in tf_nightly, which\n",
    "# can causes a duplicate tensorboard install, leading to errors.\n",
    "# !pip uninstall --yes tensorboard tb-nightly\n",
    "\n",
    "# !pip install --upgrade tensorflow-federated-nightly\n",
    "# !pip install --upgrade nest-asyncio\n",
    "# !pip install --upgrade tb-nightly\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import cidds_001 as utils\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and shuffle week1\n",
    "week1 = pd.read_feather('saved_dfs/cidds-001/traffic/OpenStack/CIDDS-001-internal-week1-cleaned.feather')\n",
    "week1_shuffled = week1.sample(frac=1, random_state=13).reset_index(drop=True)\n",
    "\n",
    "# load and shuffle week2\n",
    "week2 = pd.read_feather('saved_dfs/cidds-001/traffic/OpenStack/CIDDS-001-internal-week2-cleaned.feather')\n",
    "week2_shuffled = week2.sample(frac=1, random_state=13).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_balanced = utils.get_balanced_cidds(week1_shuffled)\n",
    "week2_balanced = utils.get_balanced_cidds(week2_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect training of week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attack_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>---</th>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bruteForce</th>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dos</th>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pingScan</th>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portScan</th>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "attack_type       \n",
       "---           1626\n",
       "bruteForce    1626\n",
       "dos           1626\n",
       "pingScan      1626\n",
       "portScan      1626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(week1_balanced.groupby(by='attack_type').size(), columns=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect week2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attack_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>---</th>\n",
       "      <td>2731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bruteForce</th>\n",
       "      <td>2731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dos</th>\n",
       "      <td>2731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pingScan</th>\n",
       "      <td>2731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portScan</th>\n",
       "      <td>2731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count\n",
       "attack_type       \n",
       "---           2731\n",
       "bruteForce    2731\n",
       "dos           2731\n",
       "pingScan      2731\n",
       "portScan      2731"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(week2_balanced.groupby(by='attack_type').size(), columns=['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess datasets\n",
    "* drop unused columns\n",
    "* split datasets in features and labels and one hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_x = week1_balanced.drop(columns=utils.columns_to_drop + ['attack_type'])\n",
    "week1_y = pd.get_dummies(week1_balanced['attack_type'])\n",
    "\n",
    "week2_x = week2_balanced.drop(columns=utils.columns_to_drop + ['attack_type'])\n",
    "week2_y = pd.get_dummies(week2_balanced['attack_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.z_score_normalization(week1_x, utils.columns_to_normalize)\n",
    "_ = utils.z_score_normalization(week2_x, utils.columns_to_normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split week1 and week2 in training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_week1, x_test_week1, y_train_week1, y_test_week1 = train_test_split(week1_x, week1_y, test_size=0.2, random_state=13)\n",
    "x_train_week2, x_test_week2, y_train_week2, y_test_week2 = train_test_split(week2_x, week2_y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert everything to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_week1 = x_train_week1.to_numpy()\n",
    "x_test_week1  = x_test_week1.to_numpy()\n",
    "x_train_week2 = x_train_week2.to_numpy()\n",
    "x_test_week2  = x_test_week2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_week1 = y_train_week1.to_numpy()\n",
    "y_test_week1  = y_test_week1.to_numpy()\n",
    "y_train_week2 = y_train_week2.to_numpy()\n",
    "y_test_week2  = y_test_week2.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `federated_train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 20\n",
    "SHUFFLE_BUFFER = 100\n",
    "PREFETCH_BUFFER = 10\n",
    "\n",
    "def preprocess(dataset):\n",
    "    def batch_format_fn(x, y):\n",
    "        return collections.OrderedDict(\n",
    "            x=x,\n",
    "            y=y\n",
    "        )\n",
    "    \n",
    "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
    "        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_client_data():\n",
    "    client_data_dict = {\n",
    "        'client_1': tf.data.Dataset.from_tensor_slices((x_train_week1, y_train_week1)),\n",
    "        'client_2': tf.data.Dataset.from_tensor_slices((x_train_week2, y_train_week2))\n",
    "    }\n",
    "\n",
    "    client_data = tff.simulation.ClientData.from_clients_and_fn(\n",
    "        client_ids=['client_1', 'client_2'],\n",
    "        create_tf_dataset_for_client_fn=lambda key: client_data_dict[key]\n",
    "    )\n",
    "\n",
    "    return client_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_federated_data(client_data, client_ids):\n",
    "    return [\n",
    "        preprocess(client_data.create_tf_dataset_for_client(x))\n",
    "        for x in client_ids\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data = make_client_data()\n",
    "client_ids = client_data.client_ids\n",
    "federated_train_data = make_federated_data(client_data, client_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the element_spec of the input that the federated model will receive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_example_data = federated_train_data[0]\n",
    "tff_input_element_spec = preprocessed_example_data.element_spec\n",
    "\n",
    "# sample_batch = tf.nest.map_structure(\n",
    "#     lambda x: x.numpy(),\n",
    "#     next(iter(preprocessed_example_data))\n",
    "# )\n",
    "# sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to create the TFF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(16,)),\n",
    "        tf.keras.layers.Dense(100, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(100, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=tff_input_element_spec,\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.CategoricalCrossentropy()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model on federated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bram_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( -> <model=<trainable=<float32[16,100],float32[100],float32[100,100],float32[100],float32[100,5],float32[5]>,non_trainable=<>>,optimizer_state=<int64>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(iterative_process.initialize.type_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.8156759), ('categorical_crossentropy', 0.5535047), ('loss', 0.553455)]))])\n",
      "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.87873536), ('categorical_crossentropy', 0.34057474), ('loss', 0.3405747)]))])\n",
      "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.8936195), ('categorical_crossentropy', 0.30198026), ('loss', 0.3019388)]))])\n",
      "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9014919), ('categorical_crossentropy', 0.28104463), ('loss', 0.28078523)]))])\n",
      "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.90898556), ('categorical_crossentropy', 0.26548192), ('loss', 0.2653803)]))])\n",
      "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.91375947), ('categorical_crossentropy', 0.2535985), ('loss', 0.25359464)]))])\n",
      "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.918499), ('categorical_crossentropy', 0.2424446), ('loss', 0.24229793)]))])\n",
      "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9207826), ('categorical_crossentropy', 0.23627007), ('loss', 0.23627006)]))])\n",
      "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.92324996), ('categorical_crossentropy', 0.22869109), ('loss', 0.22869079)]))])\n",
      "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9259582), ('categorical_crossentropy', 0.2230743), ('loss', 0.22307433)]))])\n",
      "round 11, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9275304), ('categorical_crossentropy', 0.21733189), ('loss', 0.21733189)]))])\n",
      "round 12, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.92719764), ('categorical_crossentropy', 0.21491005), ('loss', 0.21491)]))])\n",
      "round 13, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9301469), ('categorical_crossentropy', 0.21091504), ('loss', 0.21091501)]))])\n",
      "round 14, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9311338), ('categorical_crossentropy', 0.207105), ('loss', 0.20710497)]))])\n",
      "round 15, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9306633), ('categorical_crossentropy', 0.20868623), ('loss', 0.20868623)]))])\n",
      "round 16, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9322584), ('categorical_crossentropy', 0.20361306), ('loss', 0.2031463)]))])\n",
      "round 17, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.93414044), ('categorical_crossentropy', 0.19970642), ('loss', 0.19935986)]))])\n",
      "round 18, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9339224), ('categorical_crossentropy', 0.19822417), ('loss', 0.19822417)]))])\n",
      "round 19, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.93527657), ('categorical_crossentropy', 0.19496395), ('loss', 0.19496047)]))])\n",
      "round 20, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.93598807), ('categorical_crossentropy', 0.19280174), ('loss', 0.19280173)]))])\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 20\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "  state, metrics = iterative_process.next(state, federated_train_data)\n",
    "  print('round {:2d}, metrics={}'.format(round_num+1, metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the global model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate test sets from week1 and week2 to obtain a bigger test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.concatenate([x_test_week1, x_test_week2])\n",
    "test_y = np.concatenate([y_test_week1, y_test_week2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the federated trained weights to a model that can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_keras_model()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.CategoricalCrossentropy()])\n",
    "state.model.assign_weights_to(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the test set and create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lbl = np.argmax(test_y, axis=1)\n",
    "pred_y_lbl = np.argmax(pred_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[863,  23,   5,   2,   2],\n",
       "       [105, 667,   0,   6,  45],\n",
       "       [  1,   0, 905,   0,   0],\n",
       "       [ 38,  27,   0, 810,   3],\n",
       "       [ 11,  75,   0,  35, 734]])>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(labels=y_lbl, predictions=pred_y_lbl, num_classes=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47dc783c64eba16578308ded2496eecd3f77c9cbf1d7b090711421ceeb34a226"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
